[
  {
    "path": "posts/2021-02-24-web-scraping/",
    "title": "Web Scraping",
    "description": "Creating a function to use regular expressions while retrieving data of interest from HMTL code, in order to build a data frame. Then creating a function that collects and aggregates data across multiple URL's.",
    "author": [
      {
        "name": "Adam Bandola",
        "url": {}
      }
    ],
    "date": "2021-02-20",
    "categories": [],
    "contents": "\n\nContents\nBackground Context\nCreating Regular Expression Function\nFunction to Aggregate Data\nRunning a T-Test\nFinal Conclusion\n\nBackground Context\nI am tasked with convincing my group that an expert-driven approach is a better way to predict NFL final standings per conference, when compared to a model-based approach.\nThe website used for data was Probability Football. If looking at the URL it is important to note that:  * The Home Team is always on top of the Away Team * The winner of the game is always in bold * The “Pick” column reports the average probability reported by experts before the game\nCreating Regular Expression Function\nIn this part, I am scraping data from probabilityfootball.com, which is a website that houses data on football game results, in order to measure the accuracy of experts in their weekly pick of winners.\n\nShow Code\n#function that is going to be applied to weeks 5:21\nscrap_football_data <-function(url) {\n  library(\"httr\")\n  html_code <- GET(url)\n  html_code <- content(html_code, \"text\", encoding = \"ISO-8859-1\")\n  html_code <- gsub(x = html_code, pattern = \"[[:cntrl:]]\", replacement = \"\")\n  html_code <- gsub(x = html_code, pattern = \"[[:blank:]]+\", replacement = \" \") \n  \n  #Getting Team Names\n  Team_Names <- gregexpr(pattern = \"TARGET=\\\"_BLANK\\\">.+?<\/TD>\", text=html_code)\n  matches    <- regmatches(x=html_code, m=Team_Names)\n  Team_Names <- matches[[1]]\n  Team_Names <- substr(x=Team_Names, start=17, nchar(Team_Names)-9) #removing the first html tags and the last to only have which teams won left\n  Team_Names <- unique(head(Team_Names,-2)) # Removing the last two because they're from the tie breakers section \n  Team_Names <- gsub(x=Team_Names, pattern=\"<B>|<\/B>\", replacement=\"\")\n  \n  #Getting which teams won next\n  Winning_Team <- gregexpr(pattern = \"TARGET=\\\"_BLANK\\\"><B>.+?<\/B><\/A><\/TD>\", html_code)\n  matches      <- regmatches(x=html_code, m=Winning_Team)\n  Winning_Team <- matches[[1]]\n  Winning_Team <- substr(x=Winning_Team, start=20, nchar(Winning_Team)-13)\n  Winning_Team <- unique(head(Winning_Team, -1)) #Removing the last one because its from the tie breaker section \n  \n  #Getting the home team by using a sequence\n  Home_Team <- Team_Names[seq(from=1, to=length(Team_Names), by=2)]\n  #Getting the away team by using a sequence\n  Away_Team <- Team_Names[seq(from=2, to=length(Team_Names), by=2)]\n  \n  #Getting percentages\n  Prediction_Teams <- gregexpr(pattern=\"align=center><NOBR>.+?<\/NOBR><\/TD>\", text=html_code)\n  matches          <- regmatches(x=html_code, m=Prediction_Teams)\n  Prediction_Teams <- matches[[1]]\n  Prediction_Teams <- substr(x=Prediction_Teams, start=20, nchar(Prediction_Teams)-12)\n  Prediction_Teams <- gsub(x=Prediction_Teams, pattern=\"<B>|<\/B>\", replacement=\"\")\n  Prediction_Teams <- gsub(x=Prediction_Teams, pattern=\"%\", replacement=\"\")\n  \n  #Getting prediction for home and away using sequence again\n  Prediction_Home_Team <- Prediction_Teams[seq(from=1, to=length(Prediction_Teams), by=2)]\n  Prediction_Away_Team <- Prediction_Teams[seq(from=2, to=length(Prediction_Teams), by=2)]\n  \n  #Creating data frame with vectors\n  result <- data.frame(Home_Team, Away_Team, Winning_Team, Prediction_Home_Team, Prediction_Away_Team)\n  result\n  return(result)\n}\n\n\n\n\nThe above function receives a URL as an arugment and returns a data frame called result containing data in five columns, namely Home_Team, Away_Team, Winner_Team, Prediction_Home_Team, and Prediction_Away_Team.\n\nFunction to Aggregate Data\nSince the URL stays the same except the weeknum=, I can create a function that will go through the URL pages and aggregate the data for each week into a data frame. I did so with a for function and specified that it select weeks five through twenty one, which is all the regular season weeks.\n\nShow Code\n#creating a for loop to run the function over each week of the url, having it rbind the rows from each result into another data frame\nFootball_data <- NULL\nfor (week in 5:21){\n  url <- paste0(\"https://probabilityfootball.com/picks.html?1487349677&username=AVERAGES&weeknum=\", week, \"\")\n  week <- scrap_football_data(url)\n  Football_data <- rbind(Football_data, week)\n}\n\nlibrary(knitr)\nkable(head(Football_data), caption = \"First Six Results From Football_Data\")\n\n\nTable 1: First Six Results From Football_Data\nHome_Team\nAway_Team\nWinning_Team\nPrediction_Home_Team\nPrediction_Away_Team\nNew Orleans\nIndianapolis\nIndianapolis\n31\n69\nAtlanta\nMinnesota\nMinnesota\n39\n61\nCarolina\nSt. Louis\nCarolina\n45\n55\nDenver\nBuffalo\nDenver\n60\n40\nKansas City\nHouston\nHouston\n48\n52\nMiami\nWashington\nWashington\n45\n55\n\n\nThe table above is the first six rows for the football_data data frame, the data frame contains 256 rows total. Prediction_Home_Team and Prediction_Away_Team are percentages, summing the numbers for each row will give 100%.\n\nRunning a T-Test\nIn order to answer the question of whether or not experts can accurately produce foreasts, something needs to be compared to the expert-driven approach. For this, a unbiased coin that would accurately predict the winner of NFL games fifty percent of the time.\n\nShow Code\nprobability_winner <- ifelse(Football_data$Winning_Team == Football_data$Home_Team, \n                             as.character(Football_data$Prediction_Home_Team), \n                             as.character(Football_data$Prediction_Away_Team))\n\nFootball_data$probability_winner <- probability_winner\n\nFootball_data$probability_winner <- as.integer(Football_data$probability_winner) # was character before, but we need it to be integer in order to do t test\n\nt_test <- t.test(Football_data$probability_winner, mu=50, alternative = \"greater\")\n\nlibrary(pander)\npander(t_test)\n\n\nOne Sample t-test: Football_data$probability_winner\nTest statistic\ndf\nP value\nAlternative hypothesis\nmean of x\n7.355\n255\n1.303e-12 * * *\ngreater\n57.74\n\n\nNull Hypothesis: Mu=50  Alternative Hypthesis: Mu > 0  where Mu is the mean accuracy of predictions by experts\n\nFrom the output of the t-test, we can see that, on average, experts will predict the winner of NFL games more often than an unbiased coin (50% of the time) would.\nFinal Conclusion\nWhile the expert-driven approach did, on average, beat the unbiased coin, the reason for conducting this experiment was to prove that an expert-driven approach is better to use than a model-based approach. Based on the findings, I would be skeptical to recommend this approach over others without the ability to directly compare.  For that reason, I would refrain from saying this is the best way to go about predicting NFL standings, but would likely reference the fact that it is fast, simple and easy to replicate.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-17T18:05:25-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-04-other-projects/",
    "title": "Decision Tree Modeling",
    "description": "In this post we are looking to lower the difference in area under the curve (AUC) between the training and validation data. We do so by experimenting with the minimum node size and the number of variables randomly sampled at each split.",
    "author": [],
    "date": "2020-05-08",
    "categories": [],
    "contents": "\n\nContents\nPartitioning The Data\nCreating Our Random Forest Model\nAUC Level Analysis\n\nPartitioning The Data\n\nThe data was partitioned using an 80/20 approach where eighty percent of the original dataset became the training data the model was built on and the remaining twenty percent became the validation data the model was tested on. I partitioned the data because I needed to test the model on data that was not included (“fresh” data so to speak), without having the ability to obtain more data.\n\n\nShow Code\npart2data <- readRDS(\"PART 1.RDS\") # reading in data that was pre-prepared\n# str(part2data) # inspecting to see if all of the data is correctly structured, commented out because the report is redundant when viewed more than once\nlibrary(dplyr)\npart2data <- select(part2data, -DivisionIS) # removing the DivisionIS variable from part2data\n\nset.seed(33) # setting a seed so we are able to accurately compared\nindex <- sample(1:nrow(part2data), round(0.8*nrow(part2data)), replace = FALSE)\ntraining <- part2data[index,] #creating training and validation sets of data, from the index, for building the model and testing the model\nvalid <- part2data[-index,]\n\n\n\nCreating Our Random Forest Model\n\nShow Code\n#Creating and running the model\ncvindx <-createFolds(index, k=10, returnTrain = TRUE)\nctrl.f <- trainControl(method=\"cv\", index=cvindx, summaryFunction = twoClassSummary, classProbs = TRUE)\n\ntunegrid3 <- expand.grid(\n    .mtry = c(5, 10, 15),\n    .splitrule = \"gini\",\n    .min.node.size = c(550, 600, 650)\n)\n\ncl.random3 <- parallel::makeCluster(3, setup_strategy = \"sequential\")  # speeding up the process of the model by specifying the number of cores that are going to work on producing the model\nregisterDoParallel(cl.random3)\n\nrandom.forest3 <- train(retained~., data=training, method=\"ranger\", tuneGrid=tunegrid3, metric=\"ROC\", \n               num.trees=500, importance=\"impurity\", trControl=ctrl.f )\nstopCluster(cl.random3) # need this otherwise the function will continue on forever \nsaveRDS(random.forest3, \"ProjectsDT1-RandomForestModel.RDS\")\n\n\n\n\nShow Code\nrandom.forest3 <- readRDS(\"ProjectsDT1-RandomForestModel.RDS\")\nplot(random.forest3)\n\n\n\n\nAUC Level Analysis\n\nThe model elicits a reasonable training AUC of 0.723 and an acceptable validation AUC of 0.590. While it would be preferred to have a smaller gap between the two AUC’s, I can conclude that the model is not overfit to the data (which would have indicated that new data would not have performed well with the model).\n\n\nIn the future, if I wanted to control the overfitting more, I could adjust the minimum node size to help shrink the trees or change the number of variables that will be subset since most of the models were choosing 15.\n\n\nShow Code\n#Finding the training AUC\np.rforest.t3 <- predict(random.forest3, data=training, type=\"prob\")\ntraining_vector <- as.vector(training$retained)\nrt3 <- roc(training_vector, p.rforest.t3[,2])\nr.forest.auc.t3 <- rt3$auc\n\n#Finding the validation AUC\np.rforest.v3 <- predict(random.forest3, newdata=valid, type=\"prob\")\nr3 <- roc(valid$retained, p.rforest.v3[,2])\nr.forest.auc.v3 <- r3$auc\n\n#Creating a table that has the AUC values for the training and validation sets of data\naucType <- c(\"Training\", \"Validation\") # creating the categories for the data sets\naucValues <- c(r.forest.auc.t3, r.forest.auc.v3) # creating a variable for both AUC's\naucData <- as.data.frame(c(aucType, aucValues)) \naucData$AUC <- aucData[3:4,] # creating another column from the first two rows which were \"Training\" and \"Validation\" \naucData$Data <- aucData[1:2,1]\naucData <- aucData[-c(3:4),] # removing the non important rows\naucData <- aucData[,-c(1)] # removing the first column as its repetitive to AUC column\n\naucData %>%\n  kbl(caption = \"Table Representation of AUC\") %>%\n  kable_classic(aucData, lightable_options=\"basic\", full_width = F, html_font = \"Cambria\") \n\n\n\nTable 1: Table Representation of AUC\n\n\nAUC\n\n\nData\n\n\n0.723604621195497\n\n\nTraining\n\n\n0.590249401940536\n\n\nValidation\n\n\n\n\n\n",
    "preview": "posts/2021-02-04-other-projects/ProjectsDT-DecisionTree_files/figure-html5/PlotRF-1.png",
    "last_modified": "2021-03-02T19:15:24-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-03-welcome/",
    "title": "Data Pre-Processing",
    "description": "In this post is data pre-processing for a retention predictive modeling project. Includes variable recoding, imputation and dummy variables.",
    "author": [],
    "date": "2020-02-20",
    "categories": [],
    "contents": "\n\nContents\nBackground Context\nCreating New Dataset\nAddressing Response Variable\nRemoving Variables\nRecoding High School Type\nImputating Variables With Missing Values\nCreating Dummy Variables\n\nBackground Context\nIn this project for a class, I was tasked by Dr. Scott Sportsman, the Director of Enrollment Research and Analysis, who had given us a task of assigning a likelihood of as student not to be retained based on only the application data of the student. The university definition of retention is that a student returned for their third semester. Thus the retention rate reported in US News and World Report is the percentage of students who start as typical freshman and returned for the first semester of their sophomore year. Note that “return” means that the student was enrolled in class on October 15 of that semester. If a student left Miami after that date in the semester then they would be counted as being “retained”. This will give Miami information as to which students to give extra attention to upon admittance.\nGoal\nTo clean data for later processing into logistic regression, random and boosted tree models, and neural network models.\nCreating New Dataset\n\nShow Code\n#-----------------------------------------------Creating New Dataset--------------------------------------------------------------\nlibrary(dplyr) \n#Creating a new data set that only has the variables that are included in the 2018 year, ulimately testing our model on\n#Below are the variables we used and, if they weren't working based on given names, we found names of variables that matched up:\n              #Left out SAT R ERW, SAT R Math\n              #Changed Class Percentile to RankPercentile because they seemed to be the same, just two different names\n              #High School Type Slate = HsType\n              #Could not find Parent 1 or 2 Education Level in the data set as a variable\n              #TOEFL could not be found in the data set along with TOEFL Total\n              #TOEFL Listening = TIBL, TOEFL Reading = TIBR, TOEFL Written Expression = TIBW, TOEFL Speaking = TIBS\n              #IELTS Overall = IELO, IELTS Listening = IELL... so on\n              #Assumed that VentureScholar was = SummerScholars\ndata <- select(data, retained, tag, HomeState, ZipCode, Zip5, NationDesc, CountyDesc, Status, ACEFlag, InternationalFlag, StateResidency, Gender, MCFlag, OneRace, FullRace, FirstGen, AlumniConnection, Division, Major, SuppMajor1, SuppMajor2, Concentration, ApplicationType, ApplicationDate,Housing, SpecialConsideration, DisciplinaryQuestion1, Decision, DecisionType, DecisionDate, ConfirmDate, ACTBest, GPA, AcadRS, GPAScale, GPAOrig, ClassRank, ClassSize, ACTComposite, ACTEng, ACTMath, ACTRdng, ACTSci, ACTWRSC, SATVerbal, SATMath, SATWRSC, ACTChoice, ACTMaxComposite, ACTMaxEnglish, ACTMaxMath, ACTMaxReading, ACTMaxSciReasoning, ACTWritingMax, RankPercent, HighSchoolCode, HighSchoolState, HsType, EER, DateFrom, Citizen, Citizenship, TIBL, TIBR, TIBW, TIBS, IELO, IELL, IELR, IELW, IELS, Bridges, VentureScholar, Race, Hispanic, AS_Race, AI_Race, BL_Race, HS_Race, PI_Race, MiamiRanks)\n\n\n#----------------------------------------------Selecting Our Training Years------------------------------------------------------\ndata <- subset(data, DateFrom >= 2013) #Subsetting the data to include the years that we are interested in\n#Helps when figuring out what variables to remove considering were only looking at the observations in years were interested in\nvariable_removal <- summary(as.factor(data$DateFrom)) \n\n\n\n\nIndividually selected the variables that was wanted to include in the data frame. Some variables were left out, some were renamed. Also subsetted the data in order to only include the years that were of interest.\n\nAddressing Response Variable\nRecoding the response variable to ensure that it is in the preferred format.\n\nShow Code\n#--------------------------------------------------Response Variable Retained----------------------------------------------------------------\n#Looking at our response variable, retained\n# class(data$retained) #It is an integer, but we want it as a factor, so:\ndata$retained <- as.factor(data$retained)\n# levels(data$retained)\n# summary(data$retained)\ndata$retained <- recode_factor(data$retained, \"0\" = \"Yes\",  \"1\" = \"No\")\n# levels(data$retained)\n# summary(data$retained) #The base is 0 (retained) and 1 is that they are not retained, 23 missing values that we remove:\n\n# Delete the retained that are missing - You can't use rows that have the response missing\nM_retained <- which(is.na(data$retained))\ndata <- data[-M_retained,]\n\n\n\n\nChanged the base of retained variable to be 0 (Yes, the student is retained) and 1 (No, the student was not retained). Proceeded to remove the rows that have missing retained values.\n\nRemoving Variables\nNeed to remove variables that have too many missing values as well as removed variables that are redundant, not important or unclear.\n\nShow Code\n#----------------------------------------------------Removing Variable Work----------------------------------------------------------\n##Finding out the variables that have over 38% missing data, then removing them\nlibrary(DataExplorer)\nplot1 <- plot_missing(data)\n\ndata <- data[, colMeans(is.na(data)) <= .38]\ndata <- select(data, -c(Citizen, Race))\ndata <- select(data, -HighSchoolState) # Have other variables that account for this\ndata <- select(data, -OneRace)  #Decide to remove OneRace because it has more missing observations than any single level inside of the variable\ndata <- select(data, -Concentration) # Keep major, delete concentration, they account for the same variable \ndata <- select(data, -Citizenship) # Already have the variable citizen, redundant variable \ndata <- select(data, -tag)  #Removing Tag because we are not concerned with the ID of the student that dropped\ndata <- select(data, -Decision) #You cannot retain a student that is not admitted to the university, therefore it also has only one level that is AA = Admitted \ndata <- select(data, -HighSchoolCode)  #Removing HighSchoolCode because we have so many other variables that account for where the person lives, the type of highschool, etc. It's redundant. \ndata$GPA[data$GPA >5] <- mean(data$GPA)# Delete observations where GPA is greater than 5 because that isn't possible\ndata <- select(data, -EER)  #Decided to delete EER because the definition was very unclear and wouldn't be easy to comprehend in a model.\ndata <- select(data, -SuppMajor1, -SuppMajor2) #Aren't interested in what majors they selected on their application\ndata <- select(data, -FullRace) #Very unclear definition, if its in regards to race then the variable is redundant\ndata <- select(data, -GPAScale) #GPA is already accounted for on a scale of 4.0, redundant variable\ndata <- select(data, -HomeState) #Redundant since CountyDesc accounts for the county and state someones from\ndata <- select(data, -ACTChoice) #Not interested in which ACT test they chose\ndata <- select(data, -MiamiRanks) #Doesn't apply to what were interested in\ndata <- select(data, -CountyDesc) #Already have StateResidency that is in the data, redundant variable\n\nplot2 <- plot_missing(data)\n\n\n\n\nFigure 1: Before and After\n\n\n\n\nRemoved variables that had more than 38% of their observations missing. Normally, I would have stuck closer to 50% of the data missing, but there was a large drop off from 38% to the next highest so I decided to not include those that had over 38%. Many of the variables were redundant, such as Concentration is redundant to Major, so the lesser important of the two was removed. Same goes for many of the variables being removed in this section.\n\nRecoding High School Type\nRecoding the HsType to be more condensed and logical.\n\nShow Code\n#-----------Recoding Variables to make more sense/decrease degrees of freedom needed for dummy variables---------------------\nplot_bar(data$HsType)\ndata$HsType <- recode_factor(data$HsType, \"1\" = \"Public\", \"2\" = \"Private\", \"3\" = \"Catholic\", \"4\" = \"Other Parochial\")\nlevels(data$HsType) # A lot of the levels dont have many observations and can be combined into an \"Other category\"\n\n\n [1] \"Public\"          \"Private\"         \"Catholic\"       \n [4] \"Other Parochial\" \"\"                \"Private Secular\"\n [7] \"Religious\"       \"Unknown\"         \"Charter\"        \n[10] \"Home School\"    \nShow Code\n                    # Catholic and other religions could be grouped together to just be called a religious school\n        data$HsType <- recode_factor(data$HsType, \"Catholic\" = \"Religious\", \"Other Parochial\" = \"Religious\") #Religious\n        data$HsType <- recode_factor(data$HsType, \"Unknown\" = \"Other\", \"Charter\" = \"Other\", \"Home School\" = \"Other\") #Other small ones\n        data$HsType <- recode_factor(data$HsType, \"Private Secular\" = \"Private\")\nplot_bar(data$HsType)        \n\n\n\n\nFigure 2: Before and After\n\n\n\n\nMany of the variables are broken into two parts prior to the recoding. 1 is combined with Public, 2 is combined with Private, 3 is combined with Catholic, 4 is combined with Other Parochial. From this step, I combined the religious schools into a single category, forms of high school type that were smaller (Charter, Home School) into a single category and combined Private Secular into Private category.\n\nImputating Variables With Missing Values\nAfter the 38% threshold of missing values, there are still some variables that have missing values. For this reason, imputation for both numerical and categorical variables is necessary.\n\nShow Code\n#---------------------------------------------------Imputation--------------------------------------------------------------\n#Beginning to impute variables that have some percent of the observations missing, the highest percent being 26%. \n# str(data)\n\nplot3 <- plot_missing(data)\n## NUMERICAL\n#ClassSize\ndata$Indicator_ClassSize <- as.factor(ifelse(is.na(data$ClassSize), 1, 0))\ndata$ClassSize[is.na(data$ClassSize)]<-median(data$ClassSize, na.rm=TRUE)\n#ACTEng\ndata$Indicator_ACTEng <- as.factor(ifelse(is.na(data$ACTEng), 1, 0))\ndata$ACTEng[is.na(data$ACTEng)]<-median(data$ACTEng, na.rm=TRUE)\n#ACTSci\ndata$Indicator_ACTSci <- as.factor(ifelse(is.na(data$ACTSci), 1, 0))\ndata$ACTSci[is.na(data$ACTSci)]<-median(data$ACTSci, na.rm=TRUE)\n#ACTRdng\ndata$Indicator_ACTRdng <- as.factor(ifelse(is.na(data$ACTRdng), 1, 0))\ndata$ACTRdng[is.na(data$ACTRdng)]<-median(data$ACTRdng, na.rm=TRUE)\n#ACTMath\ndata$Indicator_ACTMath <- as.factor(ifelse(is.na(data$ACTMath), 1, 0))\ndata$ACTMath[is.na(data$ACTMath)]<-median(data$ACTMath, na.rm=TRUE)\n#ACTComposite\ndata$Indicator_ACTComposite <- as.factor(ifelse(is.na(data$ACTComposite), 1, 0))\ndata$ACTComposite[is.na(data$ACTComposite)]<-median(data$ACTComposite, na.rm=TRUE)\n# GPA Imputing\ndata$M_GPA <- as.factor(ifelse(is.na(data$GPA), 1, 0))\ndata$GPA[is.na(data$GPA)] <- mean(data$GPA, na.rm = TRUE)\n#AcadRS\ndata$Indicator_AcadRS = as.factor(ifelse(is.na(data$AcadRS), 1, 0))\ndata$AcadRS[is.na(data$AcadRS)] = median(data$AcadRS, na.rm = TRUE)\n\nplot4 <- plot_missing(data)\n\n## CATEGORICAL \n#HsType\nlevels <- levels(data$HsType)\nlevels[length(levels) + 1] <- \"Unknown\"\ndata$HsType <- factor(data$HsType, levels = levels)\ndata$HsType[is.na(data$HsType)] <- \"Unknown\"\n#DecisionDate\nlevels <- levels(data$DecisionDate)\nlevels[length(levels) + 1] <- \"Unknown\"\ndata$DecisionDate <- factor(data$DecisionDate, levels = levels)\ndata$DecisionDate[is.na(data$DecisionDate)] <- \"Unknown\"\n\n#ApplicationDate\nlevels <- levels(data$ApplicationDate)\nlevels[length(levels) + 1] <- \"Unknown\"\ndata$ApplicationDate <- factor(data$ApplicationDate, levels = levels)\ndata$ApplicationDate[is.na(data$ApplicationDate)] <- \"Unknown\"\n\n\n\n\nFigure 3: Numerical and Categorical Variables with Missing Values\n\n\n\nCreating Dummy Variables\nCreating dummy variables for categorical variables so logistic regression can be used on the data.\n\nShow Code\n#-----------------------------------------Creating Dummy Variables for Categorical----------------------------------------\n# str(data)\n\n# InternationalFlag needs consistent formatting (caps vs. non-cap) then turned into dummy variable ****\n# summary(data$InternationalFlag)\ndata$InternationalFlag <- recode_factor(data$InternationalFlag, \"domestic\" = \"Domestic\", \"international\" = \"International\")\n# levels(data$InternationalFlag)\n# summary(data$InternationalFlag)\ndumIntFlag <- model.matrix(~0+InternationalFlag, data=data)\ndumIntFlag <- as.data.frame(dumIntFlag)\ndata$DomesticFlag <- as.factor(dumIntFlag$InternationalFlagDomestic)\ndata <- select(data, -InternationalFlag) #Created the dummy so we have no need for the original variable now\n\n#Division\ndummyDivision <- model.matrix(~ 0 + Division, data = data)\ndummyDivision <- as.data.frame(dummyDivision)\ndata$DivisionAP <- as.factor(dummyDivision$DivisionAP)\ndata$DivisionAS <- as.factor(dummyDivision$DivisionAS)\ndata$DivisionBU <- as.factor(dummyDivision$DivisionBU)\ndata$DivisionCAS <-as.factor(dummyDivision$DivisionCAS)\ndata$DivisionCCA <- as.factor(dummyDivision$DivisionCCA)\ndata$DivisionCEC <- as.factor(dummyDivision$DivisionCEC)\ndata$DivisionCEHS <- as.factor(dummyDivision$DivisionCEHS)\ndata$DivisionFSB <- as.factor(dummyDivision$DivisionFSB)\ndata$DivisionFA <- as.factor(dummyDivision$DivisionFA)\ndata$DivisionEA <- as.factor(dummyDivision$DivisionEA)\ndata <-  select(data, -Division)\n\n#StateResidency\ndummyStateResidency = as.data.frame(model.matrix(~ 0 + StateResidency, data = data))\ndummyStateResidency = as.data.frame(dummyStateResidency)\ndata$StateResidencyN = as.factor(dummyStateResidency$StateResidencyN)\ndata$StateResidencyR = as.factor(dummyStateResidency$StateResidencyR)\ndata$StateResidencyZ = as.factor(dummyStateResidency$StateResidencyZ)\n\n#Dummy variable for Gender\ndum1 <- model.matrix(~0+Gender, data = data)\ndum1 <- as.data.frame(dum1)\ndata$Female <- as.factor(dum1$GenderF)\n# summary(data)\ndata <- select(data, -Gender) # Delete gender because we have the dummy variables\n\n# Dummy variables for Application Type \n# str(data$ApplicationType)\ndum2<-model.matrix(~0+ApplicationType, data=data)\ndum2<-as.data.frame(dum2)\ndata$ApplicationTypeOE<-as.factor(dum2$ApplicationTypeOE)\ndata$ApplicationTypeOF<-as.factor(dum2$ApplicationTypeOF)\ndata$ApplicationTypeOM<-as.factor(dum2$ApplicationTypeOM)\ndata <- select(data, -ApplicationType)\n\n#Dummy variable for Major\nMajorDummy <- as.data.frame(model.matrix(~0+Major, data=data))\n#summary(data$Major) # Chose the top 5 majors that were most frequent \ndata$Major_UniversityStudies <- as.factor(MajorDummy$`MajorUniversity Studies`)\ndata$Major_ASU2 <- as.factor(MajorDummy$MajorASU2)\ndata$Major_BU14 <- as.factor(MajorDummy$MajorBU14)\ndata$Major_BU56 <- as.factor(MajorDummy$MajorBU56)\ndata$Major_Biology <- as.factor(MajorDummy$MajorBiology)\n# summary(data)\ndata <- select(data, -Major)\n\n# Delete date variables \ndata <- select(data, -ApplicationDate, -DecisionDate, -DateFrom)\n\n\n# ---------------------------------------- CODE TO SAVE AS AN RDS FILE ---------------------------------------------\nsaveRDS(data, \"clean.data.RDS\")\n# getwd() # saves to where your WD is \n\n\n\n\n\n\n",
    "preview": "posts/2021-02-03-welcome/Data-PreProcessing_files/figure-html5/RemovingVariables-1.png",
    "last_modified": "2021-03-01T16:25:40-05:00",
    "input_file": {}
  }
]
