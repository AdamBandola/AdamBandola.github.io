---
title: "Decision Tree Modeling"
description: |
  In this post we are looking to lower the difference in area under the curve (AUC) between the training and validation data. We do so by experimenting with the minimum node size and the number of variables randomly sampled at each split. 
date: 05-08-2020
output:
  distill::distill_article:
    self_contained: false
    toc: TRUE
    toc_float: TRUE
    code_folding: Show Code
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE,
                      cache = TRUE,
                      progress = FALSE, 
                      verbose = FALSE,
                      dpi = 300,
                      out.width = '100%')
setwd("/Users/adam/Desktop/Portfolio Website/_posts/2021-02-04-other-projects")
```
## Partitioning The Data 
> The data was partitioned using an 80/20 approach where eighty percent of the original dataset became the training data the model was built on and the remaining twenty percent became the validation data the model was tested on. I partitioned the data because I needed to test the model on data that was not included ("fresh" data so to speak), without having the ability to obtain more data.

```{r Partitioning, echo=TRUE, cache=TRUE, eval=TRUE}
part2data <- readRDS("PART 1.RDS") # reading in data that was pre-prepared
# str(part2data) # inspecting to see if all of the data is correctly structured, commented out because the report is redundant when viewed more than once
library(dplyr)
part2data <- select(part2data, -DivisionIS) # removing the DivisionIS variable from part2data

set.seed(33) # setting a seed so we are able to accurately compared
index <- sample(1:nrow(part2data), round(0.8*nrow(part2data)), replace = FALSE)
training <- part2data[index,] #creating training and validation sets of data, from the index, for building the model and testing the model
valid <- part2data[-index,]
```

```{r InstallPackages, include=FALSE}
#Loading in all of the necessary packages 
library(caret)
library(DataExplorer)
library(pROC)
library(ggplot2)
library(ROCR)
library(ipred)
library(rpart)
library(knitr)
library(kableExtra)
library(doParallel)
library(ranger)
library(gbm)
library(randomForest)
library(nnet)
library(NeuralNetTools)
library(magrittr)
```

## Creating Our Random Forest Model 
```{r RandomForestModel, echo=TRUE, cache=TRUE, eval=TRUE}
#Creating and running the model
cvindx <-createFolds(index, k=10, returnTrain = TRUE)
ctrl.f <- trainControl(method="cv", index=cvindx, summaryFunction = twoClassSummary, classProbs = TRUE)

tunegrid3 <- expand.grid(
    .mtry = c(5, 10, 15),
    .splitrule = "gini",
    .min.node.size = c(550, 600, 650)
)

cl.random3 <- parallel::makeCluster(3, setup_strategy = "sequential")  # speeding up the process of the model by specifying the number of cores that are going to work on producing the model
registerDoParallel(cl.random3)

random.forest3 <- train(retained~., data=training, method="ranger", tuneGrid=tunegrid3, metric="ROC", 
               num.trees=500, importance="impurity", trControl=ctrl.f )
stopCluster(cl.random3) # need this otherwise the function will continue on forever 
saveRDS(random.forest3, "ProjectsDT1-RandomForestModel.RDS")
```

```{r PlotRF, echo=FALSE, cache=TRUE, eval=TRUE}
random.forest3 <- readRDS("ProjectsDT1-RandomForestModel.RDS")
plot(random.forest3)
```
## AUC Level Analysis

> The model elicits a reasonable training AUC of 0.723 and an acceptable validation AUC of 0.590. While it would be preferred to have a smaller gap between the two AUC's, I can conclude that the model is not overfit to the data (which would have indicated that new data would not have performed well with the model).  

> In the future, if I wanted to control the overfitting more, I could adjust the minimum node size to help shrink the trees or change the number of variables that will be subset since most of the models were choosing 15. 

```{r AUCLevels, echo=TRUE, cache=TRUE, eval=TRUE}
#Finding the training AUC
p.rforest.t3 <- predict(random.forest3, data=training, type="prob")
training_vector <- as.vector(training$retained)
rt3 <- roc(training_vector, p.rforest.t3[,2])
r.forest.auc.t3 <- rt3$auc

#Finding the validation AUC
p.rforest.v3 <- predict(random.forest3, newdata=valid, type="prob")
r3 <- roc(valid$retained, p.rforest.v3[,2])
r.forest.auc.v3 <- r3$auc

#Creating a table that has the AUC values for the training and validation sets of data
aucType <- c("Training", "Validation") # creating the categories for the data sets
aucValues <- c(r.forest.auc.t3, r.forest.auc.v3) # creating a variable for both AUC's
aucData <- as.data.frame(c(aucType, aucValues)) 
aucData$AUC <- aucData[3:4,] # creating another column from the first two rows which were "Training" and "Validation" 
aucData$Data <- aucData[1:2,1]
aucData <- aucData[-c(3:4),] # removing the non important rows
aucData <- aucData[,-c(1)] # removing the first column as its repetitive to AUC column

aucData %>%
  kbl(caption = "Table Representation of AUC") %>%
  kable_classic(aucData, lightable_options="basic", full_width = F, html_font = "Cambria") 
```
